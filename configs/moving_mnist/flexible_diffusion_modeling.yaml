# Flexible Video Diffusion Modeling
diffusion:
  parameterization: "epsilon"
  # Sampling section determines the size of the sampled output
  # from the model, and defines the sampling method used
  sampling:
    output_channels: 1
    output_spatial_size: 32
    output_frames: 16
    target: video_diffusion.samplers.ancestral.AncestralSampler
    params: {}
  # The noise scheduler to use with the forward diffusion process.
  noise_scheduler:
    target: video_diffusion.scheduler.DiscreteNoiseScheduler
    params:
      schedule_type: linear
      num_scales: 1000
      loss_type: "l2"
  importance_sampler:
    target: video_diffusion.importance_sampling.UniformSampler
    params:
      num_timesteps: 1000
  # A preprocessor to use with the context before sending to the score network.
  context_preprocessing:
      # The Prompts Preprocessor converts the list of text prompts in the context
      # into a batch of text tokens of shape (B, text_context_size)
      - target: video_diffusion.context.IgnoreContextAdapter
        params: {}
  # A preprocessor for input to the model.
  input_preprocessing:
    target: video_diffusion.context.IgnoreInputPreprocessor
    params: {}
  # Setting for classifier free guidance.
  classifier_free_guidance:
    # Classifier-free guidance scale, where the value is >= 1.0
    classifier_free_guidance: 0.0
    # Unconditional guidance probability
    unconditional_guidance_probability: 0.0
    # The context signals to apply guidance to.
    signals: []
    # For classifier free guidance, we need the ability to create an unconditional
    # context given the conditional context. This unconditional context needs
    # to be applied in both training and sampling, and will return a new
    # context dictionary given the original context dictionary.
    unconditional_context:
      target: torch.nn.Identity
      params: {}
  # Defines the score network for predicting the noise parameter
  score_network:
    target: video_diffusion.score_networks.unet_factorized3d.UNet
    params:
      # The number of input channels to the model.
      input_channels: 1
      # The number of output channels to the model.
      output_channels: 1
      # The spatial size of the input to the model.
      input_spatial_size: 32
      # The number of frames in each batch
      input_number_of_frames: 16
      # The number of features/channels at the start of
      # the network. This defines the inner dimensions
      # of the model.
      model_channels: 128
      # The number of resnet blocks per resolution.
      num_res_blocks: 2
      # Attention resolutions
      attention_resolutions: [16, 8]
      # Dropout scale
      dropout: 0.1
      # Resnet block channel multipliers.
      channel_mult: [1, 2, 2, 2]
      # Perform resampling using convolutions.
      conv_resample: True
      # Use scale/shift of the GroupNorm in the timestep embedding.
      # This is also called Adaptive Group Normalization.
      use_scale_shift_norm: True
      # Dimensionality of the model.
      dims: 2
      # Number of attention heads to use
      num_heads: 4
      # Number of upsampling heads (-1 use num_heads)
      num_heads_upsample: -1
      # Use relative position encoding in the
      use_rpe_net: True
      # Does the model include a learned sigma or a fixed sigma.
      is_learned_sigma: False
      # Additional conditioning signals for the model. The projections
      # defined here will be applied before running through the rest of the
      # score network.
      conditioning:
        # The signals (keys in the dictionary) that are available in the conditioning
        # context.
        signals: ["timestep"]
        projections:
          # A projection to apply to the integer timesteps.
          timestep:
            # Defines a projection incorporating the sinusoidal position embedding.
            # Output size is (B, C, num_features * time_embedding_mult)
            target: video_diffusion.layers.embedding.TimestepEmbeddingProjection
            params:
              num_features: 128
              time_embedding_mult: 4
              max_time: 1000.0
        # The context transformer to use at the top of the score network. This transforms
        # the context with a shared set of parameters.
        context_transformer_head:
          # Timestep -> timestep embedding
          - target: video_diffusion.layers.embedding.RunProjection
            params:
              input_context_key: "timestep"
              output_context_key: "timestep_embedding"
              projection_key: "timestep"

# Describes the dataset used in training.
data:
  # Spatial width/height of the data input to the model.
  image_size: 32
  # Number of channels in the input data
  num_channels: 1
  # The number of classes in the dataset
  num_classes: 10
  input_number_of_frames: 16

# Describes training parameters, including sampling
# strategies for input data.
training:
  # Use the batch training methodology from Flexible Diffusion Modeling
  # to train a joint conditional model of random frame/conditioning sets.
  flexible_diffusion_modeling: True
  flexible_diffusion_modeling_method: "uniform"